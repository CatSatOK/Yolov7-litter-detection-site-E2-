{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1e60c6-8e67-4f56-9d77-8fdc764ca026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076c7ff4-43e8-4722-9452-b37d023db6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TACO'...\n",
      "remote: Enumerating objects: 668, done.\u001b[K\n",
      "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
      "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
      "remote: Total 668 (delta 63), reused 75 (delta 51), pack-reused 580\u001b[K\n",
      "Receiving objects: 100% (668/668), 93.36 MiB | 2.56 MiB/s, done.\n",
      "Resolving deltas: 100% (440/440), done.\n",
      "Updating files: 100% (25/25), done.\n"
     ]
    }
   ],
   "source": [
    "#download TACO\n",
    "!git clone https://github.com/pedropro/TACO.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7edf7c04-326f-42db-b39c-6c4e008752d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/TACO\n"
     ]
    }
   ],
   "source": [
    "#check in taco\n",
    "!pwd\n",
    "\n",
    "cd TACO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af24e1a-8dec-40c3-9afe-d0311285e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing requirements and getting images\n",
    "!pip install -r requirements.txt\n",
    "!python download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29490b-ddc6-4c41-b1f6-42509b4902bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply split_dataset to randomly split data\n",
    "%cd detector/\n",
    "!python split_dataset.py --dataset_dir ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "384bac31-3d1b-4e89-ae9a-1a3dee8ae2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/notebooks/TACO/data')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assign variable to taco data\n",
    "DATA_TACO = Path.cwd()/\"data\"\n",
    "DATA_TACO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e54ba25-8d9b-413c-9f6e-f806b6140329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create image directories\n",
    "def build_split_images_and_json_file(path_dir_annotations:Path, list_dir_split:List[str]):\n",
    "    \"\"\"Build a mapping directeries of yalov7. \n",
    "    This funtion create images directories of train, test and val directeries.\n",
    "    Also it create the annotations json files of splits directories\n",
    "\n",
    "    Args:\n",
    "        path_dir_annotations (Path): It's a path of annotations directory (e.g TACO/data/annotations)\n",
    "        list_dir_split (List[str]): split's names list (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    # project directory\n",
    "    project_dir = Path.cwd().parent\n",
    "    # create a new data directory contains the splits directories train, test and val\n",
    "    datasets = project_dir/\"datasets\"\n",
    "    datasets.mkdir(exist_ok=True)\n",
    "    # Get a list all of json files annatations\n",
    "    file_json = [f for f in path_dir_annotations.parent.iterdir() if f.is_file() and str(f).endswith(\".json\")]\n",
    "    # Get a list all of images batchs directories\n",
    "    dir_imgs = [d for d in path_dir_annotations.iterdir() if d.is_dir()]\n",
    "    # Get a parent of images directories\n",
    "    path_dir_image_batch=dir_imgs[0].parent\n",
    "    #print(path_dir_image_batch)\n",
    "    # Iterate over the list that contains the names of created folders\n",
    "    for annot in list_dir_split:\n",
    "        print(annot)\n",
    "        #Dictionary of the new specific annotations json file\n",
    "        dict_js = {}\n",
    "        # Name of split folder (e.g \"datasets/train\")\n",
    "        dir_split = datasets/annot\n",
    "        dir_split.mkdir(exist_ok=True)\n",
    "        images_dir = dir_split/\"images\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        # Get a list all of json files annotations of specific split directory\n",
    "        file_split = [f for f in file_json if f.is_file() and str(f).__contains__(annot)]\n",
    "\n",
    "        # Iterate over annotations json files list\n",
    "        for fic in file_split:\n",
    "            with open(fic,\"r\", encoding=\"utf-8\") as f:\n",
    "                json_load = json.load(f)\n",
    "            # update dictionary with the contains annotations json files \n",
    "            dict_js.update(json_load)\n",
    "        for el in dict_js[\"images\"]:\n",
    "            file_name = el[\"file_name\"]\n",
    "            path_img = path_dir_image_batch/file_name\n",
    "            \n",
    "            file_out = \"_\".join(file_name.split(\"/\"))\n",
    "            output_file = dir_split/\"images\"/file_out\n",
    "            el[\"file_name\"]=f\"{annot}/images/{file_out}\"\n",
    "            if not output_file.exists():\n",
    "                output_file.write_bytes(path_img.read_bytes())\n",
    "            \n",
    "        with open(dir_split/f\"annotations-{annot}.json\",\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict_js,f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2b1081f-4f4b-4bc6-8011-316c727e6bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/TACO/data/annotations\n"
     ]
    }
   ],
   "source": [
    "# Assign a variable directories' split names\n",
    "split_list_dir = [\"train\",\"test\",\"val\"]\n",
    "# Assign variable Annotations directory\n",
    "annotations = DATA_TACO/\"annotations\"\n",
    "\n",
    "# Run the function\n",
    "build_split_images_and_json_file(annotations,split_list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc890899-e927-4452-a94b-3d6dc5ef61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to checking count of each images directories\n",
    "def check_count_imgs_in_split_dir(list_path_images_split_dir:List[Path]):\n",
    "    for img in list_path_images_split_dir:\n",
    "        name_dir = img.parent.name\n",
    "        list_path = [f for f in img.iterdir() if f.is_file()]\n",
    "        print(f\"count images of {name_dir} folder is : {len(list_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4fafc43-7900-4656-94ca-351ef01b54c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count images of train folder is : 1200\n",
      "count images of test folder is : 150\n",
      "count images of val folder is : 150\n"
     ]
    }
   ],
   "source": [
    "datasets = DATA_TACO.parent.parent/\"datasets\"\n",
    "train_images_dir = datasets/\"train\"/\"images\"\n",
    "test_images_dir = datasets/\"test\"/\"images\"\n",
    "val_images_dir = datasets/\"val\"/\"images\"\n",
    "list_split_pathImgs = [train_images_dir,test_images_dir,val_images_dir]\n",
    "\n",
    "#calling check function\n",
    "check_count_imgs_in_split_dir(list_split_pathImgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "804a9f58-1f43-48a0-bf17-75b2beb13c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox normalisation function\n",
    "\n",
    "def normalise_bbox(df):\n",
    "        bbox = df[\"bbox\"].values[0]\n",
    "        print(bbox[0])\n",
    "        W = df[\"width\"].values[0]\n",
    "        H = df[\"height\"].values[0]\n",
    "        x = bbox[0] \n",
    "        y = bbox[1] \n",
    "        w = bbox[2]\n",
    "        h = bbox[3]\n",
    "        X = np.round((x + w/2)/W,6)\n",
    "        Y = np.round((y + h/2)/H,6)\n",
    "        wn = np.round(w/W,6)\n",
    "        hn = np.round(h/H,6)\n",
    "        return [X,Y,wn, hn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "680857f4-3d7f-442d-82dd-cc54a9fafaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segmenation normalisation function\n",
    "def normalize_segmentation(seg_values):\n",
    "    seg_value = seg_values['segmentation'].values[0][0]\n",
    "    width = seg_values[\"width\"].values\n",
    "    height = seg_values[\"height\"].values\n",
    "    \n",
    "    list_seg = []\n",
    "    \n",
    "    for i, v in enumerate(seg_value):\n",
    "        if i % 2:\n",
    "            \n",
    "            y = np.round(float(v)/float(height[0]),6)\n",
    "            \n",
    "            list_seg.append(y)\n",
    "        else:\n",
    "            \n",
    "            #print(\"width ===>\",type(width[0]))\n",
    "            x = np.round(float(v)/float(width[0]),6)\n",
    "            list_seg.append(x)\n",
    "    return list_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9857f120-df97-4d8b-b5a2-486afe8f4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating labels for each directory\n",
    "def build_labels_txt_with_segmentations(data_path_dir:Path,names_dir:List[str]):\n",
    "    \"\"\"Create a folders for each split directory (e.g train, test, val), in each folder we create labels folder\n",
    "    These labels folders contains txt files \n",
    "\n",
    "    Args:\n",
    "        data_path_dir (Path): Path of dataset directory for all of mapping data for yolov7\n",
    "        names_dir (List[str]): The list names of each splits directories (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    #Iterate over names split diretories list\n",
    "    for name in names_dir:\n",
    "        path_annotations_dir = datasets/name\n",
    "\n",
    "        #Get transformed annotations json file in this directory\n",
    "        path_annotations = [f for f in path_annotations_dir.iterdir() if str(f).endswith(\".json\")][0]\n",
    "        # Create labels directory\n",
    "        labelsTrain_path = data_path_dir/name/\"labels\"\n",
    "        labelsTrain_path_supcat = data_path_dir/name/\"labels_supCat\"\n",
    "        labelsTrain_path_supcat.mkdir(exist_ok=True)\n",
    "        labelsTrain_path.mkdir(exist_ok=True, parents=True)\n",
    "        #Get data from annotations json file\n",
    "        with open(path_annotations, \"r\", encoding=\"utf-8\") as f:\n",
    "            annotates = json.load(f)\n",
    "        #Create DataFrame from data json file\n",
    "        images = pd.DataFrame(annotates[\"images\"], columns=[\"id\",\"file_name\",\"width\",\"height\"])\n",
    "        images.rename(columns={\"id\":\"image_id\"}, inplace=True)\n",
    "        annot = pd.DataFrame(annotates[\"annotations\"], columns=[\"id\",\"image_id\",\"category_id\",\"segmentation\",\"bbox\"])\n",
    "        cat = pd.DataFrame(annotates[\"categories\"]).rename(columns={\"id\":\"category_id\"}).sort_values(by=\"category_id\", ascending=True)\n",
    "        df = annot.merge(images)\n",
    "        df = df.merge(cat)\n",
    "    \n",
    "        cat_index = cat.supercategory.unique()\n",
    "        df.supercategory_id = np.nan\n",
    "\n",
    "        for i,v in enumerate(cat_index):\n",
    "            df.loc[df['supercategory']==v, \"supercategory_id\"]= i\n",
    "        # Loop to create  labels txt files\n",
    "        for img in df[\"image_id\"].unique():        \n",
    "            seg = df[df[\"image_id\"]==img]\n",
    "            length = len(seg.index)\n",
    "            i = 0\n",
    "            name_file = Path(seg['file_name'].values[0])\n",
    "            labelsTrain_supcat = labelsTrain_path_supcat/f\"{name_file.stem}.txt\"\n",
    "            path_txt = labelsTrain_path/f\"{name_file.stem}.txt\"\n",
    "            for j in range(length):\n",
    "                labels_seg = seg.iloc[i:j+1,:]\n",
    "                seg_value = labels_seg['segmentation'].values[0][0]\n",
    "                seg_zn = normalize_segmentation(labels_seg)\n",
    "                #print(\"values ===>\",seg_value)\n",
    "                coord_seg = \",\".join([str(x)for x in seg_zn]).replace(\",\",\" \")\n",
    "                lab_seg = f\"{labels_seg['category_id'].values[0]} {coord_seg}\\n\"\n",
    "                #print(labels_seg[\"bbox\"])\n",
    "                bbox = labels_seg[\"bbox\"].values[0]\n",
    "                bbox_nz = normalise_bbox(labels_seg)\n",
    "\n",
    "                #print(bbox_nz)\n",
    "                #print(labels_seg[\"width\"])\n",
    "                coord_bbox = \",\".join([str(x) for x in bbox_nz]).replace(\",\",\" \")\n",
    "                lab_bbox = f\"{int(labels_seg['supercategory_id'].values[0])} {coord_bbox}\\n\"\n",
    "                i+=1\n",
    "                with open(labelsTrain_supcat,\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(lab_bbox)\n",
    "\n",
    "                with open(path_txt,\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(lab_seg)\n",
    "    print(\"==================finished===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401125ce-8177-499e-af7e-b5218930d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling\n",
    "build_labels_txt_with_segmentations(datasets,[\"train\",\"test\",\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0adeef4a-5cf9-4e13-a853-b3d755b3b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create the ymal\n",
    "def create_yaml_taco_data():\n",
    "    cur_dir = Path.cwd()\n",
    "    taco_data = cur_dir/\"data\"\n",
    "    yolov7_dir = cur_dir.parent/\"yolov7\"\n",
    "    yolov7_data = yolov7_dir/\"data\"\n",
    "    yaml_data = yolov7_data/\"taco.yaml\"\n",
    "    yaml_data_sup = yolov7_data/\"taco_sup.yaml\"\n",
    "    global_annotations_path = [f for f in taco_data.iterdir() if str(f).endswith(\"annotations.json\")][0]\n",
    "    with open(global_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        global_annotations = json.load(f)\n",
    "        categories = pd.DataFrame(global_annotations[\"categories\"]).rename(columns={\"id\":\"category_id\"}).sort_values(by=\"category_id\")\n",
    "        annotations_global = pd.DataFrame(global_annotations[\"annotations\"],columns=[\"category_id\"]).merge(categories).sort_values(by=\"category_id\")\n",
    "        \n",
    "    classes = list(annotations_global[\"name\"].unique())\n",
    "    nc = len(annotations_global[\"name\"].unique())\n",
    "    nc_sup = len(annotations_global[\"supercategory\"].unique())\n",
    "    classes_sup = list(annotations_global[\"supercategory\"].unique())\n",
    "    dict_taco_yaml={\n",
    "        \"train\": \"../datasets/train/images\",\n",
    "        \"val\": \"../datasets/val/images\",\n",
    "        \"test\": \"../datasets/test/images\",\n",
    "        \n",
    "        \"nc\": nc,\n",
    "        \"names\":classes,    \n",
    "    }\n",
    "    dict_taco_sup_yaml={\n",
    "        \"train\": \"../datasets/train/images\",\n",
    "        \"val\": \"../datasets/val/images\",\n",
    "        \"test\": \"../datasets/test/images\",\n",
    "        \n",
    "        \"nc\": nc_sup,\n",
    "        \"names\":classes_sup,    \n",
    "    }\n",
    "    with open(yaml_data_sup, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(dict_taco_sup_yaml,f, indent=4)\n",
    "    with open(yaml_data, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(dict_taco_yaml,f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8754fe85-2f99-44eb-bf36-3cb097f8d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling\n",
    "create_yaml_taco_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88314a-e63a-4920-9ccb-e19e1f71ef87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110b142-6bab-4991-bb8d-7db251060071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a90d773-e09b-4dbd-aafe-f534fa502e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055d3a0-f8a6-4266-b745-249c5ae8e983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b0e684f-fe35-41f6-9230-db3614027e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e7b60db-a833-4d19-8a34-6daf6d63988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/yolov7/data\n"
     ]
    }
   ],
   "source": [
    "cd yolov7/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8ebfeff-9e75-4669-b931-93117449b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path.cwd()\n",
    "yaml_taco = data/\"taco.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55b0452d-f3db-4a83-907a-21c67664d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "taco_y =yaml.safe_load(open(yaml_taco, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d25a70c-6c0c-4754-8048-133c6006268c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['Aluminium foil',\n",
       "  'Battery',\n",
       "  'Aluminium blister pack',\n",
       "  'Carded blister pack',\n",
       "  'Other plastic bottle',\n",
       "  'Clear plastic bottle',\n",
       "  'Glass bottle',\n",
       "  'Plastic bottle cap',\n",
       "  'Metal bottle cap',\n",
       "  'Broken glass',\n",
       "  'Food Can',\n",
       "  'Aerosol',\n",
       "  'Drink can',\n",
       "  'Toilet tube',\n",
       "  'Other carton',\n",
       "  'Egg carton',\n",
       "  'Drink carton',\n",
       "  'Corrugated carton',\n",
       "  'Meal carton',\n",
       "  'Pizza box',\n",
       "  'Paper cup',\n",
       "  'Disposable plastic cup',\n",
       "  'Foam cup',\n",
       "  'Glass cup',\n",
       "  'Other plastic cup',\n",
       "  'Food waste',\n",
       "  'Glass jar',\n",
       "  'Plastic lid',\n",
       "  'Metal lid',\n",
       "  'Other plastic',\n",
       "  'Magazine paper',\n",
       "  'Tissues',\n",
       "  'Wrapping paper',\n",
       "  'Normal paper',\n",
       "  'Paper bag',\n",
       "  'Plastic film',\n",
       "  'Six pack rings',\n",
       "  'Garbage bag',\n",
       "  'Other plastic wrapper',\n",
       "  'Single-use carrier bag',\n",
       "  'Polypropylene bag',\n",
       "  'Crisp packet',\n",
       "  'Spread tub',\n",
       "  'Tupperware',\n",
       "  'Disposable food container',\n",
       "  'Foam food container',\n",
       "  'Other plastic container',\n",
       "  'Plastic glooves',\n",
       "  'Plastic utensils',\n",
       "  'Pop tab',\n",
       "  'Rope & strings',\n",
       "  'Scrap metal',\n",
       "  'Shoe',\n",
       "  'Squeezable tube',\n",
       "  'Plastic straw',\n",
       "  'Paper straw',\n",
       "  'Styrofoam piece',\n",
       "  'Unlabeled litter',\n",
       "  'Cigarette'],\n",
       " 'nc': 59,\n",
       " 'test': '../datasets/test/images',\n",
       " 'train': '../datasets/train/images',\n",
       " 'val': '../datasets/val/images'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taco_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865d4e2-fcf7-4fea-8a43-30a3da437abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9712fae-4c90-4bb6-8b38-02229f87d2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d336bb66-7eaf-48cd-8eeb-c27b84752edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/yolov7\n",
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cc394-0d93-4678-a61c-73439d309a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f1d58f-1c41-49e0-9987-9cd2e5b3283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0986ea99-247d-495d-bc5f-3d0f56261b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import shutil\n",
    "#import numpy as np\n",
    "#import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be468542-9662-4c3c-b407-1d0be7604435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#from pycocotools.coco import COCO\n",
    "#data_source = COCO(annotation_file='TACO-master/data/annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c4916-a04b-4cd6-9895-a4922e2e92bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
