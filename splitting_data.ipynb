{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping the TACO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to check in TACO directory - if not then : cd TACO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\repositoryProf\\\\Project_E2\\\\TACO'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next download the TACO dataset from http://tacodataset.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once it is downloaded:\n",
    "cd detector/\n",
    "\n",
    "!python split_dataset.py --dataset_dir ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotations are split into 30 batchs of json files:\n",
    "10 Train, 10 Test, 10 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/repositoryProf/Project_E2/TACO/data')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return to TACO directory\n",
    "cd ..\n",
    "\n",
    "#asign variable for the data\n",
    "DATA_TACO = Path.cwd()/\"data\"\n",
    "DATA_TACO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build images directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split_images_and_json_file(path_dir_annotations:Path, list_dir_split:List[str]):\n",
    "    \"\"\"Build a mapping directeries of yalov7. \n",
    "    This funtion create images directories of train, test and val directeries.\n",
    "    Also it create the annotations json files of splits directories\n",
    "\n",
    "    Args:\n",
    "        path_dir_annotations (Path): It's a path of annotations directory (e.g TACO/data/annotations)\n",
    "        list_dir_split (List[str]): split's names list (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    # project directory\n",
    "    project_dir = Path.cwd().parent\n",
    "    # create a new data directory contains the splits directories train, test and val\n",
    "    datasets = project_dir/\"datasets\"\n",
    "    datasets.mkdir(exist_ok=True)\n",
    "    # Get a list all of json files annatations\n",
    "    file_json = [f for f in path_dir_annotations.parent.iterdir() if f.is_file() and str(f).endswith(\".json\")]\n",
    "    # Get a list all of images batchs directories\n",
    "    dir_imgs = [d for d in path_dir_annotations.iterdir() if d.is_dir()]\n",
    "    # Get a parent of images directories\n",
    "    path_dir_image_batch=dir_imgs[0].parent\n",
    "    print(path_dir_image_batch)\n",
    "    # Iterate over the list that contains the names of created folders\n",
    "    for annot in list_dir_split:\n",
    "        print(annot)\n",
    "        #Dictionary of the new specific annotations json file\n",
    "        dict_js = {}\n",
    "        # Name of split folder (e.g \"datasets/train\")\n",
    "        dir_split = datasets/annot\n",
    "        dir_split.mkdir(exist_ok=True)\n",
    "        images_dir = dir_split/\"images\"\n",
    "        images_dir.mkdir(exist_ok=True)\n",
    "        # Get a list all of json files annotations of specific split directory\n",
    "        file_split = [f for f in file_json if f.is_file() and str(f).__contains__(annot)]\n",
    "\n",
    "        # Iterate over annotations json files list\n",
    "        for fic in file_split:\n",
    "            with open(fic,\"r\", encoding=\"utf-8\") as f:\n",
    "                json_load = json.load(f)\n",
    "            # update dictionary with the contains annotations json files \n",
    "            dict_js.update(json_load)\n",
    "        for el in dict_js[\"images\"]:\n",
    "            file_name = el[\"file_name\"]\n",
    "            path_img = path_dir_image_batch/file_name\n",
    "            \n",
    "            file_out = \"_\".join(file_name.split(\"/\"))\n",
    "            output_file = dir_split/\"images\"/file_out\n",
    "            el[\"file_name\"]=f\"{annot}/images/{file_out}\"\n",
    "            if not output_file.exists():\n",
    "                output_file.write_bytes(path_img.read_bytes())\n",
    "            \n",
    "        with open(dir_split/f\"annotations-{annot}.json\",\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict_js,f,indent=4)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Function build_split_images_and_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\repositoryProf\\Project_E2\\TACO\\data\\annotations\n",
      "train\n",
      "test\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "split_list_dir = [\"train\",\"test\",\"val\"]\n",
    "annotations = DATA_TACO/\"annotations\"\n",
    "\n",
    "# Run Function\n",
    "build_split_images_and_json_file(annotations,split_list_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the image transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function checking count of images in directory\n",
    "def check_count_imgs_in_split_dir(list_path_images_split_dir:List[Path]):\n",
    "    for img in list_path_images_split_dir:\n",
    "        name_dir = img.parent.name\n",
    "        list_path = [f for f in img.iterdir() if f.is_file()]\n",
    "        print(f\"count images of {name_dir} folder is : {len(list_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DATA_TACO.parent.parent/\"datasets\"\n",
    "train_images_dir = datasets/\"train\"/\"images\"\n",
    "test_images_dir = datasets/\"test\"/\"images\"\n",
    "val_images_dir = datasets/\"val\"/\"images\"\n",
    "list_split_pathImgs = [train_images_dir,test_images_dir,val_images_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count images of train folder is : 1200\n",
      "count images of test folder is : 150\n",
      "count images of val folder is : 150\n"
     ]
    }
   ],
   "source": [
    "check_count_imgs_in_split_dir(list_split_pathImgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the labels directory for each split directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_txt_with_segmentations(data_path_dir:Path,names_dir:List[str]):\n",
    "    \"\"\"Create a folders for each split directory (e.g train, test, val), in each folder we create labels folder\n",
    "    These labels folders contains txt files \n",
    "\n",
    "    Args:\n",
    "        data_path_dir (Path): Path of dataset directory for all of mapping data for yolov7\n",
    "        names_dir (List[str]): The list names of each splits directories (e.g [\"train\",\"test\",\"val\"])\n",
    "    \"\"\"\n",
    "    #Iterate over names split diretories list\n",
    "    for name in names_dir:\n",
    "        path_annotations_dir = datasets/name\n",
    "\n",
    "        #Get transformed annotations json file in this directory\n",
    "        path_annotations = [f for f in path_annotations_dir.iterdir() if str(f).endswith(\".json\")][0]\n",
    "        # Create labels directory\n",
    "        labelsTrain_path = data_path_dir/name/\"labels\"\n",
    "        labelsTrain_path.mkdir(exist_ok=True, parents=True)\n",
    "        #Get data from annotations json file\n",
    "        with open(path_annotations, \"r\", encoding=\"utf-8\") as f:\n",
    "            annotates = json.load(f)\n",
    "        #Create DataFrame from data json file\n",
    "        images = pd.DataFrame(annotates[\"images\"], columns=[\"id\",\"file_name\"])\n",
    "        images.rename(columns={\"id\":\"image_id\"}, inplace=True)\n",
    "        annot = pd.DataFrame(annotates[\"annotations\"], columns=[\"id\",\"image_id\",\"category_id\",\"segmentation\",])\n",
    "        df = annot.merge(images)\n",
    "        # Loop to create  labels txt files\n",
    "        for img in df[\"image_id\"].unique():\n",
    "            seg = df[df[\"image_id\"]==img]\n",
    "            length = len(seg.index)\n",
    "            i = 0\n",
    "            name_file = Path(seg['file_name'].values[0])\n",
    "\n",
    "            path_txt = labelsTrain_path/f\"{name_file.stem}.txt\"\n",
    "            for j in range(length):\n",
    "                labels = seg.iloc[i:j+1,:]\n",
    "                seg_value = labels['segmentation'].values[0][0]\n",
    "                coord = \",\".join([str(x) for x in seg_value]).replace(\",\",\" \")\n",
    "                lab = f\"{labels['category_id'].values[0]} {coord}\\n\"\n",
    "                i+=1\n",
    "                with open(path_txt,\"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(lab)\n",
    "    print(\"==================finished===========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'file_name'], dtype='object')\n",
      "Index(['image_id', 'file_name'], dtype='object')\n",
      "Index(['image_id', 'file_name'], dtype='object')\n",
      "==================finished===========================\n"
     ]
    }
   ],
   "source": [
    "build_labels_txt_with_segmentations(datasets,[\"train\",\"test\",\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_TACO/\"annotations.json\",\"r\", encoding=\"utf-8\") as f:\n",
    "    load_json= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.DataFrame(load_json[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "supercategory=cat.groupby(\"supercategory\").name.apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcat = pd.DataFrame({k:[v] for k,v in supercategory.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_taco_data():\n",
    "    cur_dir = Path.cwd()\n",
    "    taco_data = cur_dir/\"data\"\n",
    "    yolov7_dir = cur_dir.parent/\"yolov7\"\n",
    "    yolov7_data = yolov7_dir/\"data\"\n",
    "    yaml_data = yolov7_data/\"taco.yaml\"\n",
    "    global_annotations_path = [f for f in taco_data.iterdir() if str(f).endswith(\"annotations.json\")][0]\n",
    "    with open(global_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        global_annotations = json.load(f)\n",
    "        categories = pd.DataFrame(global_annotations[\"categories\"], columns=[\"id\",\"name\"]).rename(columns={\"id\":\"category_id\"})\n",
    "        annotations_global = pd.DataFrame(global_annotations[\"annotations\"],columns=[\"category_id\"]).merge(categories).sort_values(by=\"category_id\")\n",
    "    classes = list(annotations_global[\"name\"].unique())\n",
    "    nc = len(annotations_global[\"name\"].unique())\n",
    "\n",
    "    dict_taco_yaml={\n",
    "        \"train\": \"../datasets/train/images\",\n",
    "        \"val\": \"../datasets/val/images\",\n",
    "        \"test\": \"../datasets/test/images\",\n",
    "        \n",
    "        \"nc\": nc,\n",
    "        \"names\":classes,    \n",
    "    }\n",
    "    with open(yaml_data, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(dict_taco_yaml,f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yaml_taco_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f041d930f62173bda57140748705d746e0dd4225ea085e215e0166a72756a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
